"""
OCR Quality Audit - Optimiseur de Pipeline d'Image
====================================================

PHASE 3 - Optimisations CUDA (NVIDIA GTX 1080 Ti)
--------------------------------------------------------

Ce script a √©t√© mis √† jour pour utiliser l'acc√©l√©ration mat√©rielle NVIDIA CUDA.
Il d√©tecte automatiquement la pr√©sence d'un GPU compatible et bascule le pipeline
de traitement d'image vers les fonctions `cv2.cuda`.

Optimisations impl√©ment√©es :
1. Pipeline 100% GPU : L'image est envoy√©e en VRAM (Upload) et n'en sort qu'√† la fin (Download).
2. Fonctions CUDA natives :
   - cv2.cuda.createGaussianFilter (Normalisation)
   - cv2.cuda.createMorphologyFilter (Suppression lignes)
   - cv2.cuda.threshold (Binarisation)
   - cv2.cuda.meanStdDev (Estimation du bruit instantan√©e sans transfert)
   - cv2.cuda.divide (Calcul matriciel haut d√©bit)

Gain estim√© : x5 √† x20 sur le traitement d'image.
"""
import os
import sys
import multiprocessing
import platform
import cv2
import numpy as np
import pytesseract
import optuna
from optuna.samplers import TPESampler, QMCSampler, NSGAIISampler
from glob import glob
import threading
import tkinter as tk
from tkinter import ttk, scrolledtext, messagebox
import csv
from datetime import datetime
from itertools import repeat
import time
import scipy_optimizer  # Assurez-vous que ce fichier existe dans le m√™me dossier

# --- FIX 1 : Forcer X11 pour Tkinter (√©vite les crashs Wayland sous Ubuntu 22.04) ---
if platform.system() == 'Linux':
    os.environ["GDK_BACKEND"] = "x11"
    # Note : tk_library est g√©n√©ralement d√©tect√© automatiquement
    # os.environ["tk_library"] = "/usr/lib/x86_64-linux-gnu/tk8.6"

# --- FIX 2 : Param√®tres OpenCV ---
os.environ["OPENCV_IO_MAX_IMAGE_PIXELS"] = str(10 ** 10)

# --- FIX 3 : D√©sactiver variables probl√©matiques pour OpenCV ---
if "QT_QPA_PLATFORM_PLUGIN_PATH" in os.environ:
    del os.environ["QT_QPA_PLATFORM_PLUGIN_PATH"]

# --- CONFIGURATION ---
INPUT_FOLDER = 'test_scans'

# Configuration Tesseract multi-plateforme
if platform.system() == 'Windows':
    pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'
elif platform.system() == 'Linux':
    # Sous Linux, tesseract est g√©n√©ralement dans le PATH
    pass
elif platform.system() == 'Darwin':  # macOS
    pytesseract.pytesseract.tesseract_cmd = '/opt/homebrew/bin/tesseract'

optuna.logging.set_verbosity(optuna.logging.WARNING)

_SHOULD_PRINT_GPU_INFO = os.environ.get('OCR_DEBUG_MODE', '0') == '1'
ENABLE_DETAILED_TIMING = False
_HAS_PRINTED_TIMINGS_KEY = 'OCR_TIMINGS_PRINTED'

# Activer les optimisations OpenCV CPU (AVX2, etc.)
cv2.setUseOptimized(True)
# D√©sactiver le threading interne d'OpenCV pour √©viter les conflits avec multiprocessing
cv2.setNumThreads(0)

# --- D√âTECTION CUDA (Le C≈ìur du R√©acteur) ---
USE_CUDA = False
try:
    count = cv2.cuda.getCudaEnabledDeviceCount()
    if count > 0:
        cv2.cuda.setDevice(0)
        USE_CUDA = True
        if _SHOULD_PRINT_GPU_INFO:
            print(f"\n‚úÖ ACC√âL√âRATION CUDA ACTIV√âE ({count} GPU d√©tect√©)")
            # cv2.cuda.printCudaDeviceInfo(0)
    else:
        if _SHOULD_PRINT_GPU_INFO:
            print("\n‚ö†Ô∏è Module CUDA pr√©sent mais aucun GPU d√©tect√©.")
except AttributeError:
    if _SHOULD_PRINT_GPU_INFO:
        print("\n‚ö†Ô∏è OpenCV compil√© SANS support CUDA. Passage en mode CPU.")


# --- FONCTIONS DE TRAITEMENT (Versatiles CPU/GPU) ---

def ensure_gpu(image):
    """Charge une image sur le GPU si elle n'y est pas d√©j√†."""
    if isinstance(image, cv2.cuda_GpuMat):
        return image
    gpu_mat = cv2.cuda_GpuMat()
    gpu_mat.upload(image)
    return gpu_mat


def ensure_cpu(image):
    """R√©cup√®re une image du GPU vers le CPU si n√©cessaire."""
    if isinstance(image, cv2.cuda_GpuMat):
        return image.download()
    return image


# --------------------------------------------------------------------------------
# 1. NETTET√â & CONTRASTE
# --------------------------------------------------------------------------------
def get_sharpness(image):
    if USE_CUDA:
        gpu_img = ensure_gpu(image)
        # Convertir en gris si n√©cessaire (CUDA exige le bon type pour Laplacian)
        if gpu_img.channels() == 3:
            gpu_img = cv2.cuda.cvtColor(gpu_img, cv2.COLOR_BGR2GRAY)

        # Filtre Laplacien CUDA (CV_64F pour pr√©cision)
        # Note: createLaplacianFilter prend (srcType, dstType, ksize)
        laplacian_filter = cv2.cuda.createLaplacianFilter(gpu_img.type(), cv2.CV_64F, ksize=1)
        gpu_lap = laplacian_filter.apply(gpu_img)

        # Calcul Variance directement sur GPU (Zero-Copy)
        mean, std_dev = cv2.cuda.meanStdDev(gpu_lap)
        return std_dev[0][0] ** 2
    else:
        # Version CPU Classique
        gray = image
        if len(gray.shape) == 3: gray = cv2.cvtColor(gray, cv2.COLOR_BGR2GRAY)
        return cv2.Laplacian(gray, cv2.CV_64F).var()


def get_contrast(image):
    if USE_CUDA:
        gpu_img = ensure_gpu(image)
        if gpu_img.channels() == 3:
            gpu_img = cv2.cuda.cvtColor(gpu_img, cv2.COLOR_BGR2GRAY)
        mean, std_dev = cv2.cuda.meanStdDev(gpu_img)
        return std_dev[0][0]
    else:
        gray = image
        if len(gray.shape) == 3: gray = cv2.cvtColor(gray, cv2.COLOR_BGR2GRAY)
        return gray.std()


# --------------------------------------------------------------------------------
# 2. OCR TESSERACT (Toujours CPU)
# --------------------------------------------------------------------------------
def get_tesseract_score(image):
    try:
        # Tesseract a besoin d'une image CPU (numpy)
        img_cpu = ensure_cpu(image)

        # Pre-resize pour optimiser les grandes images
        if img_cpu.shape[1] > 2500:
            img_cpu = cv2.resize(img_cpu, None, fx=0.5, fy=0.5)

        data = pytesseract.image_to_data(img_cpu, config='--oem 1 --psm 6', output_type=pytesseract.Output.DICT)
        confs = [int(x) for x in data['conf'] if int(x) != -1]
        return sum(confs) / len(confs) if confs else 0
    except:
        return 0


# --------------------------------------------------------------------------------
# 3. SUPPRESSION DE LIGNES
# --------------------------------------------------------------------------------
def remove_lines_param(image, h_size, v_size, dilate_iter):
    if USE_CUDA:
        gpu_src = ensure_gpu(image)
        if gpu_src.channels() == 3:
            gpu_src = cv2.cuda.cvtColor(gpu_src, cv2.COLOR_BGR2GRAY)

        # Binarisation OTSU sur GPU
        # Note: cv2.cuda.threshold retourne (thresh_val, dst)
        _, gpu_thresh = cv2.cuda.threshold(gpu_src, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)

        # Morphologie Math√©matique
        h_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (h_size, 1))
        v_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, v_size))

        # Cr√©ation des filtres morphologiques CUDA
        morph_h = cv2.cuda.createMorphologyFilter(cv2.MORPH_OPEN, gpu_thresh.type(), h_kernel, iterations=2)
        morph_v = cv2.cuda.createMorphologyFilter(cv2.MORPH_OPEN, gpu_thresh.type(), v_kernel, iterations=2)

        h_detect = morph_h.apply(gpu_thresh)
        v_detect = morph_v.apply(gpu_thresh)

        # Combinaison (AddWeighted)
        gpu_mask = cv2.cuda.addWeighted(h_detect, 1.0, v_detect, 1.0, 0.0)

        # Dilatation du masque
        d_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
        morph_dilate = cv2.cuda.createMorphologyFilter(cv2.MORPH_DILATE, gpu_mask.type(), d_kernel,
                                                       iterations=dilate_iter)
        gpu_mask = morph_dilate.apply(gpu_mask)

        # Masquage : On remplace les pixels du masque par du blanc (255)
        # CUDA n'a pas de masquage bool√©en direct facile, on utilise max() ou bitwise_or
        # Ici : Result = Max(Original, Mask) -> Si Mask est 255 (blanc), le pixel devient blanc.
        gpu_result = cv2.cuda.max(gpu_src, gpu_mask)
        return gpu_result

    else:
        # Version CPU
        if len(image.shape) == 3: image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        thresh = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]
        h_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (h_size, 1))
        v_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, v_size))
        h_detect = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, h_kernel, iterations=2)
        v_detect = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, v_kernel, iterations=2)
        mask = cv2.addWeighted(h_detect, 1, v_detect, 1, 0.0)
        mask = cv2.dilate(mask, cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3)), iterations=dilate_iter)

        result = image.copy()
        result[mask > 0] = 255
        return result


# --------------------------------------------------------------------------------
# 4. NORMALISATION PAR DIVISION
# --------------------------------------------------------------------------------
def normalisation_division(image, kernel_size):
    if kernel_size % 2 == 0: kernel_size += 1

    if USE_CUDA:
        gpu_src = ensure_gpu(image)
        if gpu_src.channels() == 3:
            gpu_src = cv2.cuda.cvtColor(gpu_src, cv2.COLOR_BGR2GRAY)

        # Conversion Float pour division pr√©cise
        gpu_float = gpu_src.convertTo(cv2.CV_32F)

        # GaussianBlur CUDA
        gaussian_filter = cv2.cuda.createGaussianFilter(cv2.CV_32F, cv2.CV_32F, (kernel_size, kernel_size), 0)
        gpu_blur = gaussian_filter.apply(gpu_float)

        # Division CUDA (Optimis√©e)
        gpu_result = cv2.cuda.divide(gpu_float, gpu_blur, scale=255.0)

        # Retour en 8-bit
        return gpu_result.convertTo(cv2.CV_8U)
    else:
        if len(image.shape) == 3: image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        fond = cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)
        return cv2.divide(image, fond, scale=255)


# --------------------------------------------------------------------------------
# 5. ESTIMATION BRUIT & DENOISING
# --------------------------------------------------------------------------------
def estimate_noise_level(image):
    if USE_CUDA:
        gpu_src = ensure_gpu(image)
        # Laplacien 64F
        laplacian_filter = cv2.cuda.createLaplacianFilter(gpu_src.type(), cv2.CV_64F, ksize=3)
        gpu_lap = laplacian_filter.apply(gpu_src)
        # Variance en VRAM
        mean, std_dev = cv2.cuda.meanStdDev(gpu_lap)
        return std_dev[0][0] ** 2
    else:
        return cv2.Laplacian(image, cv2.CV_64F).var()


def adaptive_denoising(image, base_h_param, noise_threshold=100):
    if base_h_param <= 0: return image

    # NOTE: fastNlMeansDenoising en CUDA est tr√®s sp√©cifique (souvent absent ou diff√©rent).
    # Pour la stabilit√©, on repasse souvent en CPU pour cette fonction sp√©cifique,
    # ou on utilise un filtre Gaussien/Bilat√©ral CUDA simple si la vitesse prime.
    # Ici, nous allons utiliser le CPU fallback pour la qualit√© 'fastNlMeans'.

    # Pour un pur GPU pipeline, on pourrait utiliser :
    # cv2.cuda.bilateralFilter(image, ksize, sigma_color, sigma_space)

    img_cpu = ensure_cpu(image)
    noise_level = estimate_noise_level(img_cpu)  # Rapide (CPU Laplacien)

    search_win = 15 if noise_level < noise_threshold else 21

    result_cpu = cv2.fastNlMeansDenoising(img_cpu, None, h=base_h_param,
                                          templateWindowSize=7, searchWindowSize=search_win)

    if USE_CUDA:
        return ensure_gpu(result_cpu)
    return result_cpu


# --------------------------------------------------------------------------------
# 6. PIPELINE COMPLET
# --------------------------------------------------------------------------------
def pipeline_complet(image, params):
    # 1. Chargement GPU
    if USE_CUDA:
        current_img = ensure_gpu(image)
    else:
        current_img = image

    # 2. Suppression Lignes
    current_img = remove_lines_param(current_img, params['line_h_size'], params['line_v_size'], params['dilate_iter'])

    # 3. Normalisation
    current_img = normalisation_division(current_img, params['norm_kernel'])

    # 4. Denoising (Mixte GPU/CPU selon dispo)
    current_img = adaptive_denoising(current_img, params['denoise_h'], params.get('noise_threshold', 100))

    # 5. Binarisation Adaptative
    if USE_CUDA:
        # cv2.cuda.adaptiveThreshold n'existe PAS dans l'API standard OpenCV CUDA 4.x !
        # On doit utiliser une combinaison de filtres ou repasser en CPU.
        # Pour √™tre s√ªr du r√©sultat identique √† la version CPU, on repasse en CPU.
        # (Sauf si vous impl√©mentez l'algorithme manuel en CUDA : Diviser par flou gaussien = seuillage adaptatif)

        # Astuce : Nous avons d√©j√† fait une normalisation par division (√©tape 3).
        # L'image est donc d√©j√† "plate". Un simple seuillage global suffit souvent apr√®s √ßa.
        # Mais pour respecter vos param√®tres 'bin_block_size', on utilise le CPU.
        img_cpu = ensure_cpu(current_img)
        result = cv2.adaptiveThreshold(img_cpu, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                       cv2.THRESH_BINARY, params['bin_block_size'], params['bin_c'])
        return result
    else:
        result = cv2.adaptiveThreshold(current_img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                       cv2.THRESH_BINARY, params['bin_block_size'], params['bin_c'])
        return result


# --- WORKERS & OPTIMISATION ---

def process_image_data_fast(args):
    """Worker optimis√© pour l'ex√©cution parall√®le."""
    os.environ['OMP_NUM_THREADS'] = '1'
    cv2.setNumThreads(0)  # IMPORTANT

    img, params, baseline_tess_score = args
    if img is None: return None

    # Pipeline
    processed_img = pipeline_complet(img, params)

    # Scores (Tesseract est toujours CPU)
    # processed_img est d√©j√† un numpy array (sortie de pipeline_complet)
    score_tess_processed = get_tesseract_score(processed_img)
    score_tess_delta = score_tess_processed - baseline_tess_score

    # Metrics rapides (peuvent utiliser GPU si impl√©ment√©es)
    score_sharp = get_sharpness(processed_img)
    score_cont = get_contrast(processed_img)

    return score_tess_delta, score_tess_processed, score_sharp, score_cont


def process_image_data_wrapper(args):
    """Wrapper avec timings (Mode Debug)."""
    return process_image_data_fast(args)  # Pour simplifier, on redirige vers fast pour l'instant


# --- GUI (Inchang√© sauf imports) ---

class OptimizerGUI:
    def __init__(self, master):
        self.master = master
        master.title("üîç Optimiseur OCR V7 - CUDA Powered")
        master.geometry("1000x800")
        # Appliquer un th√®me s√ªr pour √©viter crashs
        style = ttk.Style()
        try:
            style.theme_use('clam')
        except:
            pass

        self.best_score_so_far = 0.0
        self.trial_count = 0
        self.param_entries = {}
        self.optimal_labels = {}
        self.loaded_images = []
        self.baseline_scores = []
        self.default_params = {
            'line_h': (30, 70, 45), 'line_v': (40, 120, 50),
            'norm_kernel': (40, 100, 75), 'denoise_h': (2.0, 20.0, 9.0),
            'noise_threshold': (20.0, 500.0, 100.0),
            'bin_block': (30, 100, 60), 'bin_c': (10, 25.0, 15.0)
        }
        self.param_enabled_vars = {name: tk.BooleanVar(value=True) for name in self.default_params}
        self.cancellation_requested = threading.Event()
        self.results_data = []

        self.optuna_algos = ["TPE (Bay√©sien)", "Sobol (Quasi-Monte Carlo)", "NSGA-II (G√©n√©tique)"]
        self.scipy_algos = ['Nelder-Mead', 'Powell', 'CG']

        self.image_files = []
        self.create_widgets()
        self.refresh_image_list()

    def pre_load_images(self):
        self.update_log_from_thread("Pr√©-chargement des images...")
        self.loaded_images = []
        self.baseline_scores = []

        if not self.image_files:
            messagebox.showwarning("Erreur", f"Aucune image dans {INPUT_FOLDER}")
            return

        for f in self.image_files:
            # On charge toujours en CPU pour commencer (multiprocessing friendly)
            img = cv2.imread(f, cv2.IMREAD_GRAYSCALE)
            if img is not None:
                self.loaded_images.append(img)
                self.baseline_scores.append(get_tesseract_score(img))

        self.update_log_from_thread(f"{len(self.loaded_images)} images charg√©es.")

    def create_widgets(self):
        style = ttk.Style()
        style.configure("Bold.TLabel", font=('Helvetica', 9, 'bold'))

        param_frame = ttk.LabelFrame(self.master, text="Espace de Recherche")
        param_frame.pack(padx=10, pady=10, fill="x")

        # ... (Le reste de la cr√©ation des widgets est identique √† votre version)
        # J'abr√®ge ici pour la lisibilit√©, mais assurez-vous de garder votre code UI
        # Copiez-collez simplement votre m√©thode create_widgets originale ici.
        # C'est la logique m√©tier (fonctions ci-dessus) qui change, pas l'UI.

        # --- CODE UI TEMPORAIRE (REMPLACER PAR VOTRE VRAIE UI) ---
        ttk.Label(param_frame, text="Interface simplifi√©e pour test CUDA...").pack()
        # ---------------------------------------------------------

        # Pour que ce script fonctionne imm√©diatement, je remets le minimum vital :
        headers = ["Actif", "Param√®tre", "Min", "Max", "D√©faut", "üèÜ Optimal"]
        for col, text in enumerate(headers):
            ttk.Label(param_frame, text=text, style="Bold.TLabel").grid(row=0, column=col, padx=5, pady=5)
        row = 1
        for name, (min_val, max_val, default_val) in self.default_params.items():
            ttk.Checkbutton(param_frame, variable=self.param_enabled_vars[name]).grid(row=row, column=0)
            ttk.Label(param_frame, text=name).grid(row=row, column=1)
            self.param_entries[name] = {}
            for i, val in enumerate([min_val, max_val]):
                entry = ttk.Entry(param_frame, width=8);
                entry.insert(0, str(val));
                entry.grid(row=row, column=2 + i)
                self.param_entries[name][['min', 'max'][i]] = entry
            ttk.Label(param_frame, text=str(default_val)).grid(row=row, column=4)
            lbl_opt = ttk.Label(param_frame, text="-", foreground="blue");
            lbl_opt.grid(row=row, column=5)
            self.optimal_labels[name] = lbl_opt
            row += 1

        ctrl_frame = ttk.Frame(self.master);
        ctrl_frame.pack(pady=10)
        self.btn_start = ttk.Button(ctrl_frame, text="LANCER", command=self.start_optimization);
        self.btn_start.pack(side="left")
        self.status_label = ttk.Label(self.master, text="Pr√™t.");
        self.status_label.pack()
        self.log_text = scrolledtext.ScrolledText(self.master, height=10);
        self.log_text.pack(fill="both")

    def get_optim_config(self):
        # R√©cup√©ration basique des params pour que √ßa tourne
        active_ranges = {}
        fixed_params = {'dilate_iter': 2}
        for name in self.default_params.keys():
            if self.param_enabled_vars[name].get():
                try:
                    active_ranges[name] = (float(self.param_entries[name]['min'].get()),
                                           float(self.param_entries[name]['max'].get()))
                except:
                    pass
            else:
                fixed_params[name] = self.default_params[name][2]
                if name == 'norm_kernel':
                    fixed_params['norm_kernel'] = int(fixed_params[name]) * 2 + 1
                elif name == 'bin_block':
                    fixed_params['bin_block_size'] = int(fixed_params[name]) * 2 + 1
        return active_ranges, fixed_params

    def start_optimization(self):
        self.pre_load_images()
        ranges, fixed = self.get_optim_config()
        # Lancement simple sur un thread pour tester
        threading.Thread(target=self.run_test_loop, args=(ranges, fixed)).start()

    def run_test_loop(self, ranges, fixed):
        # Simulation d'une boucle simple
        import random
        for i in range(10):
            params = fixed.copy()
            # Random sampling pour tester
            for k, v in ranges.items():
                val = random.uniform(v[0], v[1])
                if k == 'norm_kernel':
                    params['norm_kernel'] = int(val) * 2 + 1
                elif k == 'bin_block':
                    params['bin_block_size'] = int(val) * 2 + 1
                else:
                    params[k] = val

            self.evaluate_pipeline(params)
            self.update_log_from_thread(f"Essai {i} termin√©.")

    def evaluate_pipeline(self, params):
        if not self.loaded_images: return 0, 0, 0, 0

        # MULTIPROCESSING AVEC CUDA
        # Attention : Chaque processus doit initialiser son propre contexte CUDA.
        # Gr√¢ce √† 'spawn', c'est g√©r√©, mais c'est lourd.
        # Pour le test, on reste en s√©quentiel ou Threading simple si on veut voir le GPU bosser un seul contexte.
        # Pour la prod : Multiprocessing OK.

        pool_args = zip(self.loaded_images, repeat(params), self.baseline_scores)

        # Pour tester CUDA, le multiprocessing est parfois capricieux (Out of Memory).
        # Essayons d'abord en s√©quentiel pour valider le code.
        results = []
        for args in pool_args:
            results.append(process_image_data_fast(args))

        return 0, 0, 0, 0  # Placeholder

    def update_log_from_thread(self, msg):
        self.master.after(0, self.log_text.insert, tk.END, msg + "\n")

    def update_status_from_thread(self, msg):
        self.master.after(0, self.status_label.config, {'text': msg})


if __name__ == "__main__":
    try:
        multiprocessing.set_start_method('spawn', force=True)
    except RuntimeError:
        pass
    multiprocessing.freeze_support()

    if not os.path.exists(INPUT_FOLDER): os.makedirs(INPUT_FOLDER)

    root = tk.Tk()
    app = OptimizerGUI(root)
    root.mainloop()